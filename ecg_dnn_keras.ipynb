{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random, math\n",
    "import sys, os, time, json\n",
    "import scipy.interpolate as interp \n",
    "import csv \n",
    "import copy\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import re\n",
    "import pdb\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(lab_value, run_num = 0, file_dict=None):\n",
    "#     ecg_directory = '/gunvant_nas_trop/ecgs/%s' % lab_value\n",
    "#     file_dict = {}\n",
    "#     fail = []\n",
    "    \n",
    "#     for filename in tqdm(os.listdir(ecg_directory)):\n",
    "#         name = filename[:-4]\n",
    "#         if name not in file_dict.keys():\n",
    "#             try:\n",
    "# #                 pdb.set_trace()\n",
    "#                 file_dict[name] = [-1,1]\n",
    "#                 ecg_raw = np.loadtxt(ecg_directory + '/' + filename, skiprows=1, delimiter=',', usecols=range(12))\n",
    "#                 len_leads = len(ecg_raw)\n",
    "#                 if len_leads == 5000:\n",
    "#                     file_dict[name][0] = ecg_raw[::2]\n",
    "#                 elif len_leads == 2500:\n",
    "#                     file_dict[name][0] = ecg_raw\n",
    "#                 else:\n",
    "#                     print('len_leads:', len_leads)\n",
    "#             except:\n",
    "#                 fail.append(filename)\n",
    "# #         if len(file_dict)>100:\n",
    "# #             break;\n",
    "\n",
    "#     print('Failed to find:', fail)\n",
    "#     print('Num ECGS extracted:',len(file_dict))\n",
    "#     np.save('trop_all_v3_dict.npy',file_dict)\n",
    "    \n",
    "    if file_dict is None:\n",
    "        file_dict = np.load('mvp.npy',allow_pickle=True,encoding='latin1')\n",
    "    #     file_dict = np.load('trop_catLh_dict.npy',allow_pickle=True,encoding='latin1')\n",
    "    #     file_dict = np.load('k_ser_all_dict.npy',allow_pickle=True,encoding='latin1')\n",
    "    #     file_dict = np.load('potassium_serum_dict.npy',allow_pickle=True,encoding='latin1')\n",
    "        file_dict = file_dict.item()\n",
    "\n",
    "    train_voltages = []\n",
    "    train_labels = []\n",
    "    val_voltages = []\n",
    "    val_labels = []\n",
    "    test_voltages = []\n",
    "    test_labels = []\n",
    "    \n",
    "    train_labels_raw = []\n",
    "    val_labels_raw = []\n",
    "    test_labels_raw = []\n",
    "#     min_max_scaler = MinMaxScaler()\n",
    "#     quantile_scaler = QuantileTransformer()\n",
    "    \n",
    "    lab_values = pd.read_csv(\"cve_uuid_mrn.csv\")\n",
    "#     lab_values = pd.read_csv(\"4h_nearest_%s_uuid.csv\" % lab_value)\n",
    "#     lab_values = pd.read_csv(\"6h_back_%s_uuid.csv\" % lab_value)\n",
    "\n",
    "#     lab_values_exclude = pd.read_csv(\"4h_nearest_trop_all_uuid.csv\")\n",
    "#     uuids_exclude = lab_values_exclude['UUID'].drop_duplicates()\n",
    "    \n",
    "#     lab_values = lab_values.sort_values(['UUID','result_time']).drop_duplicates(subset='UUID',keep='last') #done already\n",
    "#     lab_values['norm2'] = min_max_scaler.fit_transform(lab_values['ord_value'].values.reshape(-1, 1))\n",
    "#     lab_values['norm2'] = lab_values['ord_value']\n",
    "#     min_max_scaler.fit(lab_values['ord_value'].values.reshape(-1, 1))  \n",
    "\n",
    "    mrns = lab_values['mrn'].drop_duplicates()\n",
    "    lab_values = lab_values.set_index('UUID')\n",
    "    mrn_train,mrn_not_train = train_test_split(mrns,test_size=0.3, random_state= 4* (run_num+1))\n",
    "    mrn_val, mrn_test = train_test_split(mrn_not_train,test_size=0.5, random_state=4* (run_num+1))\n",
    "    \n",
    "    CUTOFF = 0.01\n",
    "    train_dems = [] # [gender, age, weight, height, bsa]\n",
    "    val_dems = []\n",
    "    test_dems = []\n",
    "    \n",
    "    \n",
    "    failed = []\n",
    "    excluded = []\n",
    "    for mrn in tqdm(mrn_train.values):\n",
    "        for key in lab_values[lab_values['mrn'] == mrn].index:\n",
    "            try:\n",
    "                label = float(lab_values.loc[key,'disease'])\n",
    "                this_ecg = file_dict[key][0]\n",
    "                if label>=0.0 and np.max(this_ecg)<15000 and np.min(this_ecg)>-15000:# and key not in uuids_exclude.values:\n",
    "\n",
    "                    train_voltages.append(this_ecg)\n",
    "\n",
    "                    if label < CUTOFF:\n",
    "                        train_labels.append([0])\n",
    "                    else:\n",
    "                        train_labels.append([1])\n",
    "\n",
    "                    train_labels_raw.append([label])\n",
    "    #                 train_labels.append(this_hr)\n",
    "                else:\n",
    "                    excluded.append(key)\n",
    "                    pass\n",
    "            except:\n",
    "                failed.append(key)\n",
    "                pass\n",
    "    \n",
    "    for mrn in tqdm(mrn_val.values):\n",
    "        for key in lab_values[lab_values['mrn'] == mrn].index:\n",
    "            try:\n",
    "                label = float(lab_values.loc[key,'disease'])\n",
    "    #             this_hr = get_hr(file_dict[key][0])\n",
    "    #             if not math.isnan(this_hr): \n",
    "                this_ecg = file_dict[key][0]\n",
    "                if label>=0.0 and np.max(this_ecg)<15000 and np.min(this_ecg)>-15000:# and key not in uuids_exclude.values:\n",
    "\n",
    "\n",
    "                    val_voltages.append(file_dict[key][0])\n",
    "\n",
    "                    if label < CUTOFF:\n",
    "                        val_labels.append([0])\n",
    "                    else:\n",
    "                        val_labels.append([1])\n",
    "                    val_labels_raw.append([label])\n",
    "    #                 val_labels.append(this_hr)\n",
    "                else:\n",
    "                    excluded.append(key)\n",
    "                    pass\n",
    "            except:\n",
    "                failed.append(key)\n",
    "                pass\n",
    "\n",
    "    for mrn in tqdm(mrn_test.values):\n",
    "        for key in lab_values[lab_values['mrn'] == mrn].index:\n",
    "            try:\n",
    "                label = float(lab_values.loc[key,'disease'])\n",
    "    #             this_hr = get_hr(file_dict[key][0])\n",
    "    #             if not math.isnan(this_hr):\n",
    "                this_ecg = file_dict[key][0]\n",
    "                if label>=0.0 and np.max(this_ecg)<15000 and np.min(this_ecg)>-15000:# and key not in uuids_exclude.values:\n",
    "                    test_voltages.append(file_dict[key][0])\n",
    "\n",
    "                    if label < CUTOFF:\n",
    "                        test_labels.append([0])\n",
    "                    else:\n",
    "                        test_labels.append([1])\n",
    "\n",
    "                    test_labels_raw.append([label])\n",
    "    #                 test_labels.append(this_hr)\n",
    "                else:\n",
    "                    excluded.append(key)\n",
    "    #                     pdb.set_trace() \n",
    "                    pass\n",
    "            except:\n",
    "                failed.append(key)\n",
    "                pass\n",
    "    \n",
    "    print('Excluded:', len(excluded))\n",
    "\n",
    "    x_train = np.array(train_voltages)\n",
    "    y_train = np.array(train_labels)\n",
    "\n",
    "    x_val = np.array(val_voltages)\n",
    "\n",
    "    y_val = np.array(val_labels)\n",
    "#     y_val = (np.array(val_labels).reshape((-1,1))  - labels_min) / (labels_max - labels_min)\n",
    "#     y_val = (np.array(val_labels).reshape((-1,1))  - labels_mean) / (labels_std)\n",
    "\n",
    "#     x_test = np.swapaxes(test_voltages,1,2)\n",
    "#     x_test = ( ( np.clip(test_voltages, global_min, global_max) - global_min ) / (global_max - global_min) ).reshape((-1,2500,12))\n",
    "    x_test = np.array(test_voltages)\n",
    "\n",
    "    y_test = np.array(test_labels)\n",
    "\n",
    "    print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, np.array(train_dems).shape, np.array(val_dems).shape, np.array(test_dems).shape)\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test, np.array(train_dems), np.array(val_dems), np.array(test_dems), train_labels_raw, val_labels_raw, test_labels_raw\n",
    "\n",
    "# def get_hr(ecg_raw):\n",
    "#     hrs = []\n",
    "#     _,_,_,_,_,_,hrs = ecg.ecg(signal=ecg_raw[:,1], sampling_rate=250, show=False)\n",
    "    \n",
    "#     return np.mean(hrs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, y_train, y_val, y_test, dems_train, dems_val, dems_test, train_labels_raw, val_labels_raw, test_labels_raw = load_data('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class ValidDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, ecgs, labels, dems=None, batch_size=32, dim=2048, n_channels=12,\n",
    "                 n_classes=60, shuffle=False, train_cats = None):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.ecgs = ecgs\n",
    "        self.ecg_inds = np.arange(ecgs.shape[0])\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.train_cats = train_cats\n",
    "        self.dems = dems\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.ecgs.shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.ecg_inds[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.ecg_inds = np.arange(self.ecgs.shape[0])\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.ecg_inds)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "#         # Initialization\n",
    "#         X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "#         y = np.empty((self.batch_size, self.n_classes), dtype=float32)\n",
    "\n",
    "        X = self.ecgs[indexes,:self.dim,:]\n",
    "        y = self.labels[indexes, :]\n",
    "#         d = self.dems[indexes,:]\n",
    "\n",
    "#         return [np.array(X),np.array(d)], y\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.ndimage import interpolation\n",
    "\n",
    "class TrainDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, ecgs, labels, dems=None, batch_size=32, dim=2048, n_channels=12,\n",
    "                 n_classes=60, shuffle=True, train_cats = None, aug_rescale=(0.85, 1.15)):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.ecgs = ecgs\n",
    "        self.ecg_inds = np.arange(ecgs.shape[0])\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        \n",
    "        self.aug_rescale = aug_rescale\n",
    "        \n",
    "        self.train_cats = train_cats\n",
    "        \n",
    "        self.dems = dems\n",
    "        self.calc_sampling_weights()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.ecgs.shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        \n",
    "        indexes = np.random.choice(self.ecg_inds, size=self.batch_size, replace=False, p=self.ind_weights)\n",
    "        \n",
    "#         indexes = self.ecg_inds[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def calc_sampling_weights(self):\n",
    "        'Initially, calculates sampling weights for each training item'\n",
    "        \n",
    "        cats, cat_counts = np.unique(self.train_cats,return_counts=True)\n",
    "        weights = np.zeros(len(cats))\n",
    "        for xx in range(len(cats)):\n",
    "            weights[xx] = 1/ ( cat_counts[xx]/len(self.train_cats)  )\n",
    "        \n",
    "        tot_weight = sum([weights[xx] * cat_counts[xx] for xx in range(len(weights))])\n",
    "        scaled_weights = []\n",
    "        for weight in weights:\n",
    "            scaled_weights.append(weight/tot_weight)\n",
    "        \n",
    "        ind_weights = np.zeros(self.ecgs.shape[0])\n",
    "        for ii, val in enumerate(self.train_cats):\n",
    "            ind_weights[ii] = scaled_weights[val]\n",
    "        \n",
    "        print('Weights', weights)\n",
    "        print('Scaled_weights',scaled_weights)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.ind_weights = ind_weights\n",
    "    \n",
    "    def augment(self, X_in):\n",
    "        'Randomly rescales input ecg and return random window of size self.dim'\n",
    "        \n",
    "        scale_f = random.random() * (self.aug_rescale[1] - self.aug_rescale[0]) + self.aug_rescale[0]\n",
    "#         print(scale_f)\n",
    "        X_new = interpolation.zoom(X_in,zoom=[scale_f,1])\n",
    "        start = random.randrange(0, X_new.shape[0]-self.dim+1)\n",
    "        end = start + self.dim\n",
    "        \n",
    "        return X_new[start:end, :]\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "#         # Initialization\n",
    "#         X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "#         y = np.empty((self.batch_size, self.n_classes), dtype=float32)\n",
    "\n",
    "        X = self.ecgs[indexes,:,:]\n",
    "        X_out = []\n",
    "        for X_in in X:\n",
    "            X_out.append(self.augment(X_in))\n",
    "#             X_out.append(X_in[:self.dim])\n",
    "        \n",
    "        X_out = np.array(X_out)\n",
    "        y = self.labels[indexes, :]\n",
    "        \n",
    "#         d = []\n",
    "#         for each in indexes:\n",
    "#             d.append(self.dems[each,:])\n",
    "#         d = self.dems[indexes,:]\n",
    "#         print(d.shape)\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "        \n",
    "#         return [np.array(X),np.array(d)], y\n",
    "        return X_out, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / (c2 + K.epsilon())\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / (c3 + K.epsilon())\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_score \n",
    "\n",
    "class IntervalEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), log_f = 'log.csv', interval=10):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.log_f = log_f\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            Y_val = np.round(self.y_val)\n",
    "            Y_pred = np.round(y_pred)\n",
    "#             print(Y_pred)\n",
    "            cm = confusion_matrix(Y_val, Y_pred)\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(self.y_val, y_pred)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            print(\"interval evaluation - epoch: {:d} - scores:\".format(epoch), cm.diagonal(), roc_auc)\n",
    "            \n",
    "            with open(self.log_f,'a') as f:\n",
    "                f.write(\"epoch: %d - accs: %s - auc: %f\\n\" % (epoch+1, ', '.join(str(x) for x in cm.diagonal()), roc_auc) )\n",
    "\n",
    "\n",
    "def build_unet(**params): # not a unet this is our modified 12 lead weston network\n",
    "    import keras\n",
    "    from keras.layers import Conv1D, BatchNormalization, Add, MaxPooling1D\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.layers.core import Dense, Activation, Flatten\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Input\n",
    "    from keras.layers import Dropout\n",
    "    \n",
    "    \n",
    "    inputs = Input(shape=params['input_shape'],\n",
    "                   dtype='float32',\n",
    "                   name='inputs')\n",
    "    \n",
    "    r = lambda: keras.regularizers.l2(params[\"l2_kern_weight\"])\n",
    "\n",
    "    layer = Conv1D(\n",
    "             filters=params[\"conv_num_filters_start\"],\n",
    "             kernel_size=params[\"conv_filter_length\"],\n",
    "             strides=1,\n",
    "             padding='same', \n",
    "             kernel_regularizer=r(), \n",
    "            # weston does not use kernel initalizer \n",
    "             kernel_initializer=params[\"conv_init\"])(inputs)\n",
    "    \n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Activation(params[\"conv_activation\"])(layer)\n",
    "\n",
    "    layer2 = Conv1D(\n",
    "             filters=params[\"conv_num_filters_start\"],\n",
    "             kernel_size=params[\"conv_filter_length\"],\n",
    "             strides=1,\n",
    "             padding='same', \n",
    "             kernel_regularizer=r(), \n",
    "            # weston does not use kernel initalizer \n",
    "             kernel_initializer=params[\"conv_init\"])(layer)\n",
    "\n",
    "    layer2 = BatchNormalization()(layer2)\n",
    "    layer2 = Activation(params[\"conv_activation\"])(layer2)\n",
    "    layer2 = Dropout(params[\"conv_dropout\"])(layer2)\n",
    "    layer2 = Conv1D(\n",
    "                 filters=params[\"conv_num_filters_start\"],\n",
    "                 kernel_size=params[\"conv_filter_length\"],\n",
    "                 strides=1,\n",
    "                 padding='same', \n",
    "                 kernel_regularizer=r(), \n",
    "                # weston does not use kernel initalizer \n",
    "                 kernel_initializer=params[\"conv_init\"])(layer2)\n",
    "    \n",
    "    layer = Add()([layer,layer2])\n",
    "    # x = tf.layers.max_pooling1d(x, pool_size=2, strides=2, padding='same')\n",
    "    layer = MaxPooling1D(pool_size=2, strides=2,padding='same')(layer)\n",
    "    \n",
    "    for i in range(1, 1 + params[\"num_middle_layers\"]):\n",
    "        layer2 = layer\n",
    "        n_filters = params[\"conv_num_filters_start\"] * 2 ** (i // params[\"conv_increase_channels_at\"])\n",
    "\n",
    "        for j in range(params[\"num_convs_per_layer\"]):\n",
    "            layer2 = BatchNormalization()(layer2)\n",
    "            layer2 = Activation(params[\"conv_activation\"])(layer2)\n",
    "            layer2 = Dropout(params[\"conv_dropout\"])(layer2)\n",
    "            layer2 = Conv1D(\n",
    "                 filters=n_filters,\n",
    "                 kernel_size=params[\"conv_filter_length\"],\n",
    "                 strides=1,\n",
    "                 padding='same', \n",
    "                 kernel_regularizer=r(), \n",
    "                 kernel_initializer=params[\"conv_init\"])(layer2)\n",
    "            \n",
    "        if i % params[\"conv_increase_channels_at\"] == 0:\n",
    "            layer = layer2\n",
    "        else:\n",
    "            layer = Add()([layer,layer2])\n",
    "\n",
    "        if i % params[\"conv_pool_at\"] == 0:\n",
    "            # add padding = 'same'\n",
    "            layer = MaxPooling1D(pool_size=2,strides=2,padding='same')(layer)\n",
    "\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Activation(params[\"conv_activation\"])(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    for i in range(params[\"hidden_layers\"]):\n",
    "        layer = Dense(params[\"hidden_size\"])(layer)\n",
    "\n",
    "    layer = Dense(params[\"num_categories\"])(layer)\n",
    "    \n",
    "    output = Activation(\"sigmoid\")(layer)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[output])\n",
    "\n",
    "    optimizer = Adam(\n",
    "        lr=params[\"learning_rate\"],\n",
    "        clipnorm=params.get(\"clipnorm\", 1))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc', f1_score])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_filename_for_saving(save_dir, run):\n",
    "    return os.path.join(save_dir,\n",
    "            \"{val_loss:.3f}-{epoch:03d}-{loss:.3f}-%d.hdf5\" % run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hists = {}\n",
    "\n",
    "rt_dir = '/volume/12_lead/models/cve_binary_sean/baseline'\n",
    "if not os.path.isdir(rt_dir):\n",
    "    print('Making: %s', rt_dir)\n",
    "    os.mkdir(rt_dir)\n",
    "\n",
    "for iii in range(1):\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test, dems_train, dems_val, dems_test, train_labels_raw, val_labels_raw, test_labels_raw = load_data('trop_all', iii)\n",
    "    K.clear_session()\n",
    "    params={\n",
    "        \"conv_subsample_lengths\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1], #15 layers\n",
    "        \"conv_filter_length\": 16,\n",
    "        \"conv_num_filters_start\": 64,\n",
    "        \"conv_init\": \"he_normal\",\n",
    "        \"conv_activation\": \"relu\",\n",
    "        \"conv_dropout\": 0.2,\n",
    "        \"conv_num_skip\": 2,\n",
    "        \"conv_increase_channels_at\": 4,\n",
    "        \"conv_pool_at\": 2,\n",
    "        \"num_categories\":1,\n",
    "\n",
    "        \"input_shape\":[2048,12],\n",
    "\n",
    "        \"l2_kern_weight\": 0.0001,\n",
    "        \"l2_bias_weight\": 0.0001,\n",
    "\n",
    "        \"hidden_layers\": 1,\n",
    "        \"hidden_size\":1024,\n",
    "\n",
    "        \"num_middle_layers\": 15,\n",
    "        \"num_convs_per_layer\": 1,\n",
    "\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 64,\n",
    "\n",
    "        \"compile\": True,\n",
    "\n",
    "    }\n",
    "    # assert len(params['conv_subsample_lengths']) == params[\"conv_filter_length\"]\n",
    "\n",
    "    gen_params = {\n",
    "        'dim': (2048),\n",
    "        'batch_size': 128,\n",
    "        'n_classes': 1,\n",
    "        'n_channels': 12,\n",
    "        'shuffle': True,\n",
    "        'train_cats': y_train.squeeze()\n",
    "    }\n",
    "\n",
    "\n",
    "    # # Generators\n",
    "    training_generator = TrainDataGenerator(x_train, y_train, **gen_params)\n",
    "    validation_generator = ValidDataGenerator(x_val, y_val, **gen_params)\n",
    "\n",
    "    # # # Generators\n",
    "    # training_generator = TrainDataGenerator(x_train[:,:2048,:], y_train, dems_train, **gen_params)\n",
    "    # validation_generator = ValidDataGenerator(x_val[:,:2048,:], y_val, dems_val, **gen_params)\n",
    "\n",
    "\n",
    "    model = build_unet(**params)\n",
    "    print(model.summary())\n",
    "\n",
    "    stopping = keras.callbacks.EarlyStopping(patience=20)\n",
    "\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        factor=0.1,\n",
    "        patience=2,\n",
    "        min_lr=1e-4 * 0.001)\n",
    "    \n",
    "    sv_dir_base = os.path.join(rt_dir, 'run%d' % iii) \n",
    "    if not os.path.isdir(sv_dir_base):\n",
    "        print('Making: %s', sv_dir_base)\n",
    "        os.mkdir(sv_dir_base)\n",
    "    \n",
    "    checkpointer = keras.callbacks.ModelCheckpoint(filepath=get_filename_for_saving(sv_dir_base, iii),\n",
    "        save_best_only=False)\n",
    "\n",
    "    ival = IntervalEvaluation(validation_data=(x_val[:,:2048,:], y_val), \n",
    "                              log_f = os.path.join(sv_dir_base,'log_binary_1_cutoff_ensemble_evals_run%d.txt' % iii), interval=1)\n",
    "\n",
    "    csv_logger = keras.callbacks.CSVLogger(os.path.join(sv_dir_base, 'log_binary_1_cutoff_ensemble_run%d.csv' % iii), append=True)\n",
    "    \n",
    "    history = model.fit_generator(training_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        epochs=30,\n",
    "                        callbacks=[checkpointer, reduce_lr, stopping, ival, csv_logger], verbose=2)\n",
    "    \n",
    "    all_hists[iii] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(x_test[:,:2048,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/models/trop_binary_sean/all_hists_cutoff_0.04.npy',  {key:value.history for key, value in all_hists.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iii=0\n",
    "x_train, x_val, x_test, y_train, y_val, y_test, dems_train, dems_val, dems_test, train_labels_raw, val_labels_raw, test_labels_raw, file_dict = load_data('trop_all', iii, file_dict)\n",
    "\n",
    "# x_train_cath, x_val_cath, x_test_cath, y_train_cath, y_val_cath, y_test_cath, dems_train_cath, dems_val_cath, dems_test_cath, train_labels_raw_cath, val_labels_raw_cath, test_labels_raw_cath, _ = load_cath_data('trop_all', iii, file_dict = None)\n",
    "# x_cath = np.concatenate( (x_train_cath, x_val_cath, x_test_cath), axis=0)\n",
    "# y_cath = np.concatenate( (y_train_cath, y_val_cath, y_test_cath), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_results = {}\n",
    "val_results['precisions'] = precisions\n",
    "val_results['recalls'] = recalls\n",
    "val_results['fscores'] = fscores\n",
    "val_results['cutoffs'] = cutoffs\n",
    "\n",
    "test_results = {}\n",
    "test_results['precisions'] = precisions_test\n",
    "test_results['recalls'] = recalls_test\n",
    "test_results['fscores'] = fscores_test\n",
    "test_results['supports'] = supports_test\n",
    "\n",
    "print(val_results)\n",
    "print(test_results)\n",
    "\n",
    "results_85 = pd.DataFrame({'val_results': val_results, 'test_results': test_results})\n",
    "results_85.to_pickle('results_85_sensitivity.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recalls = [x[0] for x in test_results['fscores']]\n",
    "print( np.mean(test_recalls), np.std(test_recalls) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, fscores, cutoffs = [], [], [], []\n",
    "precisions_test, recalls_test, fscores_test, supports_test = [], [], [], []\n",
    "for iii in range(10):\n",
    "#     iii=0\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test, dems_train, dems_val, dems_test, train_labels_raw, val_labels_raw, test_labels_raw = load_data('trop_all', iii)\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    def f1_score(y_true, y_pred):\n",
    "\n",
    "        # Count positive samples.\n",
    "        c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "        # If there are no true samples, fix the F1 score at 0.\n",
    "        if c3 == 0:\n",
    "            return 0\n",
    "\n",
    "        # How many selected items are relevant?\n",
    "        precision = c1 / c2\n",
    "\n",
    "        # How many relevant items are selected?\n",
    "        recall = c1 / c3\n",
    "\n",
    "        # Calculate f1_score\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1_score \n",
    "\n",
    "    models = glob.glob('/volume/12_lead/models/cve_binary_sean/baseline/run%d/*.hdf5' % iii)\n",
    "    print('Found: ', models)\n",
    "    # model_1 =  keras.models.load_model(\"/models/trop_binary_sean/cutoff_0.02/run%d/0.594-022-0.568-9.hdf5\" % iii, custom_objects={'f1_score': f1_score})\n",
    "    model_1 = keras.models.load_model(models[0], custom_objects={'f1_score': f1_score})\n",
    "    # model_1 =  keras.models.load_model(\"/models/trop_binary/0.400-019-0.310.hdf5\", custom_objects={'f1_score': f1_score, 'Sine': Sine, 'SIRENInitializer': SIRENInitializer})\n",
    "\n",
    "    # from keras.optimizers import Adam\n",
    "    # optimizer = Adam(\n",
    "    #         lr=params[\"learning_rate\"],\n",
    "    #         clipnorm=params.get(\"clipnorm\", 1))\n",
    "    # model.compile(loss='binary_crossentropy',\n",
    "    #                   optimizer=optimizer,\n",
    "    #                   metrics=['mean_absolute_error'])\n",
    "    # model_1.summary()\n",
    "    \n",
    "    y_predicted = model_1.predict(x_test[:,:2048,:])\n",
    "    y_predicted_val = model_1.predict(x_val[:,:2048,:])\n",
    "    precision, recall, fscore, cutoff = process_val(0.85)\n",
    "    \n",
    "    cutoffs.append(cutoff)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    fscores.append(fscore)\n",
    "    \n",
    "    print('TEST SET:')\n",
    "    print(classification_report(y_test, y_predicted>cutoff))\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_predicted>cutoff)\n",
    "    precisions_test.append(precision)\n",
    "    recalls_test.append(recall)\n",
    "    fscores_test.append(fscore)\n",
    "    supports_test.append(support)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR VAL\n",
    "\n",
    "def process_val(target_rec = 0.9):\n",
    "    from sklearn.metrics import f1_score, recall_score, precision_recall_curve, precision_recall_fscore_support\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(1):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_val[:, i], y_predicted_val[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    print(roc_auc)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_predicted_val)\n",
    "#     plt.plot(recall, precision, label='Validation')\n",
    "#     plt.xlabel('Recall')\n",
    "#     plt.ylabel('Precision')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "    scores = []\n",
    "    for x in range(1000):\n",
    "        scores.append(f1_score(y_val, (y_predicted_val>x/1000), pos_label=0))\n",
    "    print(np.max(scores), np.where(scores == np.max(scores))[0][0]/1000)\n",
    "\n",
    "    scores = []\n",
    "    recs = []\n",
    "    for x in range(1000):\n",
    "        scores.append(f1_score(y_val, (y_predicted_val>x/1000), pos_label=1))\n",
    "        this_rec = recall_score(y_val, (y_predicted_val>x/1000), pos_label=1)\n",
    "        recs.append(this_rec)\n",
    "    print(np.max(scores), np.where(scores == np.max(scores))[0][0]/100, f1_score(y_val, (y_predicted_val>np.where(scores == np.max(scores))[0][0]/100), pos_label=0))\n",
    "\n",
    "    recs = np.array(recs)\n",
    "    best_rec_i = (np.abs(recs - target_rec)).argmin()\n",
    "    best_rec = best_rec_i/1000\n",
    "\n",
    "    print(recs[best_rec_i])\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "    print(classification_report(y_val, y_predicted_val>best_rec))\n",
    "    print(confusion_matrix(y_val, y_predicted_val>best_rec))\n",
    "    print(accuracy_score(y_val, y_predicted_val>best_rec))\n",
    "    \n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_val, y_predicted_val>best_rec)\n",
    "    \n",
    "    return precision, recall, fscore, best_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(1):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_predicted[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "print(roc_auc)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#SAME BUT FOR VAL\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(1):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_val[:, i], y_predicted_val[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "print(roc_auc)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predicted_cath = model_1.predict(x_cath[:,:2048,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR TEST\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "scores = []\n",
    "for x in range(100):\n",
    "    scores.append(f1_score(y_test, (y_predicted>x/100), pos_label=0))\n",
    "print(np.max(scores), np.where(scores == np.max(scores))[0][0]/100)\n",
    "\n",
    "scores = []\n",
    "for x in range(100):\n",
    "    scores.append(f1_score(y_test, (y_predicted>x/100), pos_label=1))\n",
    "print(np.max(scores), np.where(scores == np.max(scores))[0][0]/100, f1_score(y_test, (y_predicted>np.where(scores == np.max(scores))[0][0]/100), pos_label=0))\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "print(classification_report(y_test, y_predicted>0.45))\n",
    "print(confusion_matrix(y_test, y_predicted>0.45))\n",
    "print(accuracy_score(y_test, y_predicted>0.45))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "def plot_me(ecg_in, limits = [-1500, 1500]):\n",
    "    mdf = pd.DataFrame()\n",
    "    for each in range(12):\n",
    "        mdf['ecg2'] = ecg_in[1:1000,each].flatten()\n",
    "        mdf['ecg2'].plot()\n",
    "        pyplot.ylim(limits)\n",
    "        pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_this_one(in_num):\n",
    "    print(in_num)\n",
    "    print('True     ', y_test[in_num])\n",
    "    print('Prob of being 1', y_predicted[in_num])\n",
    "    plot_me(x_test[in_num,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_these(in_list):\n",
    "    for each in in_list:\n",
    "        plot_this_one(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_these([20, 248, 437, 428, 651, 923, 1118, 1317])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_these([9274, 8819, 7350, 6208, 4995, 2669, 309, 137])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#32 channels, 12 layers\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(1):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_predicted[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inverse Scaling\n",
    "preds_raw = (min_max_scaler[1]-min_max_scaler[0])*y_predicted + min_max_scaler[0]\n",
    "y_test_raw = (min_max_scaler[1]-min_max_scaler[0])*y_test + min_max_scaler[0]\n",
    "\n",
    "for each in np.concatenate((y_predicted,y_test), axis=1):\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for each in np.concatenate( (np.array(pred_raw).flatten().reshape(-1,1), y_test_raw[:14976]), axis=1):\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(preds).flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in y_test:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.where(abs(y_test-np.mean(y_test))<1e-4)\n",
    "np.where(y_train == np.min(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[10895]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#min\n",
    "from matplotlib import pyplot\n",
    "mdf = pd.DataFrame()\n",
    "for each in range(12):\n",
    "    mdf['ecg2'] = x_train[27512,1:1000,each].flatten()\n",
    "    mdf['ecg2'].plot()\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#max\n",
    "from matplotlib import pyplot\n",
    "mdf = pd.DataFrame()\n",
    "for each in range(12):\n",
    "    mdf['ecg2'] = x_train[7815,1:1000,each].flatten()\n",
    "    mdf['ecg2'].plot()\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_raw = (min_max_scaler[1]-min_max_scaler[0])*np.array(preds).reshape((-1,1)) + min_max_scaler[0]\n",
    "y_test_raw = (min_max_scaler[1]-min_max_scaler[0])*y_test + min_max_scaler[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = pd.DataFrame()\n",
    "mdf['pred_raw'] = np.array(y_test_raw).squeeze()\n",
    "mdf['pred_raw'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = pd.DataFrame()\n",
    "mdf['pred_raw'] = np.array(pred_raw).reshape((-1,1)).squeeze()\n",
    "mdf['pred_raw'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyper_true = y_test_raw[:14592]>5.5\n",
    "hyper_pred = np.array(pred_raw).reshape((-1,1))>5.5\n",
    "\n",
    "print(np.array(hyper_true).shape)\n",
    "print(np.array(hyper_pred).shape)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "cm1 = confusion_matrix(hyper_true,hyper_pred)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "print('Accuracy', accuracy_score(hyper_true, hyper_pred))\n",
    "print(classification_report(hyper_true, hyper_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Everything Below is Old**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_raw = min_max_scaler.inverse_transform(pred)\n",
    "y_test_raw = min_max_scaler.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf = pd.DataFrame()\n",
    "mdf['pred_raw'] = np.array(pred_raw).squeeze()\n",
    "mdf['pred_raw'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf['pred_raw'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "stand_scaler = QuantileTransformer(n_quantiles=len(y_test))\n",
    "y_test_stand = stand_scaler.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf = pd.DataFrame()\n",
    "mdf['pred_raw'] = np.array(y_test_raw).squeeze()\n",
    "mdf['pred_raw'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf['pred_raw'].hist(bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test_raw>5.5)/len(y_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf = pd.DataFrame()\n",
    "mdf['pred_raw'] = np.array(pred_raw).squeeze()\n",
    "mdf['pred_raw'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf['pred_raw'].hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
